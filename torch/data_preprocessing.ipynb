{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rustworkx Graph Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement json (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for json\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqq rustworkx numpy pandas typing\n",
    "%pip install -qqq torch\n",
    "%pip install -qqq json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1b/rgchpsl92j197mwwwch1mbmw0000gn/T/ipykernel_77860/2654540550.py:7: DeprecationWarning: This package has been superseded by the `leidenalg` package and will no longer be maintained. Please upgrade to the `leidenalg` package.\n",
      "  import louvain\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import rustworkx as rx\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "#%pip install louvain\n",
    "import louvain\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VERIS_MITRE LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/nullzero/Documents/repos/opencti/kg_infosec/veris1_3_7-mappings-enterprise.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mitre_data(file_path: json) -> Dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data_dict = json.load(f)\n",
    "        return data_dict\n",
    "\n",
    "import rustworkx as rx\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def create_knowledge_graph(data: List[Dict]) -> Tuple[rx.PyGraph, Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Creates a knowledge graph from MITRE ATT&CK data.\n",
    "\n",
    "    Args:\n",
    "        data (List[Dict]): The MITRE ATT&CK data, where each dictionary represents a technique, tactic, etc.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[rx.PyGraph, Dict[str, int]]: A tuple containing the graph and a node map.\n",
    "    \"\"\"\n",
    "    G = rx.PyGraph()\n",
    "    \n",
    "    node_map = {}\n",
    "\n",
    "    for entry in data:\n",
    "\n",
    "        technique_id = entry.get('technique_id')\n",
    "        technique_name = entry.get('technique_name')\n",
    "        tactic = entry.get('tactic')\n",
    "        related_techniques = entry.get('related_techniques', [])\n",
    "        intrusion_sets = entry.get('intrusion_sets', [])\n",
    "\n",
    "\n",
    "        if technique_id not in node_map:\n",
    "            node_index = G.add_node((technique_id, technique_name, tactic))\n",
    "            node_map[technique_id] = node_index\n",
    "        else:\n",
    "            node_index = node_map[technique_id]\n",
    "        \n",
    "\n",
    "        for related_technique in related_techniques:\n",
    "            related_id = related_technique.get('technique_id')\n",
    "            related_name = related_technique.get('technique_name')\n",
    "            if related_id not in node_map:\n",
    "                related_index = G.add_node((related_id, related_name, tactic))\n",
    "                node_map[related_id] = related_index\n",
    "            else:\n",
    "                related_index = node_map[related_id]\n",
    "            \n",
    "    \n",
    "            G.add_edge(node_index, related_index, \"related\")\n",
    "        \n",
    "\n",
    "        for intrusion_set in intrusion_sets:\n",
    "            set_id = intrusion_set.get('set_id')\n",
    "            set_name = intrusion_set.get('set_name')\n",
    "            if set_id not in node_map:\n",
    "                set_index = G.add_node((set_id, set_name, \"intrusion_set\"))\n",
    "                node_map[set_id] = set_index\n",
    "            else:\n",
    "                set_index = node_map[set_id]\n",
    "            \n",
    "            # Create an undirected edge between the technique and the intrusion set\n",
    "            G.add_edge(node_index, set_index, \"associated_with\")\n",
    "    \n",
    "    return G, node_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph(G: rx.PyGraph) -> Tuple[List[int], Dict, List[float], List[float]]:\n",
    "    # Calculate node degrees\n",
    "    degrees = G.degree()\n",
    "    \n",
    "    # Identify communities\n",
    "    adj_matrix = rx.adjacency_matrix(G)\n",
    "    partition = louvain.best_partition(adj_matrix)\n",
    "    \n",
    "    # Calculate clustering coefficients\n",
    "    clustering_coeffs = rx.clustering_coefficients(G)\n",
    "    \n",
    "    # Calculate betweenness centrality\n",
    "    betweenness = rx.betweenness_centrality(G)\n",
    "    \n",
    "    return degrees, partition, clustering_coeffs, betweenness\n",
    "\n",
    "def identify_pivotal_nodes(G: rx.PyGraph, betweenness: List[float], top_n: int = 10) -> List[Tuple[int, float]]:\n",
    "    return sorted(enumerate(betweenness), key=lambda x: x[1], reverse=True)[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m\"\u001b[39m: G, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_results\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdegrees\u001b[39m\u001b[38;5;124m\"\u001b[39m: degrees, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommunities\u001b[39m\u001b[38;5;124m\"\u001b[39m: communities, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclustering_coeffs\u001b[39m\u001b[38;5;124m\"\u001b[39m: clustering_coeffs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetweenness\u001b[39m\u001b[38;5;124m\"\u001b[39m: betweenness}}\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Here you could add more analysis, such as:\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# - Identify common attack patterns\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# - Find relationships between different intrusion sets\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# - Analyze the structure of specific sub-techniques\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_mitre_data(file_path\u001b[38;5;241m=\u001b[39mfile_path)  \n\u001b[0;32m----> 4\u001b[0m     G, node_map \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_knowledge_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     total_nodes \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mnum_nodes()\n\u001b[1;32m      8\u001b[0m     degrees \u001b[38;5;241m=\u001b[39m [G\u001b[38;5;241m.\u001b[39mdegree(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m node_map]\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mcreate_knowledge_graph\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     21\u001b[0m node_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m---> 25\u001b[0m     technique_id \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtechnique_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m     technique_name \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtechnique_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     tactic \u001b[38;5;241m=\u001b[39m entry\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtactic\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "def create_knowledge_graph(file_path=file_path):\n",
    "    data = load_mitre_data(file_path=file_path)\n",
    "    G, node_map = create_knowledge_graph(data)\n",
    "    return G, node map\n",
    "\n",
    "def main(G):\n",
    "    total_nodes = G.num_nodes()\n",
    "    \n",
    "    degrees = [G.degree(node) for node in node_map]\n",
    "    communities, clustering_coeffs, betweenness = analyze_graph(G)\n",
    "    \n",
    "    # Identify pivotal nodes\n",
    "    pivotal_nodes = identify_pivotal_nodes(G, betweenness)\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Number of nodes: {G.num_nodes()}\")\n",
    "    print(f\"Number of edges: {G.num_edges()}\")\n",
    "    print(f\"Average degree: {sum(degrees) / len(degrees):.2f}\")\n",
    "    print(f\"Number of communities: {len(set(communities.values()))}\")\n",
    "    print(f\"Average clustering coefficient: {sum(clustering_coeffs) / len(clustering_coeffs):.4f}\")\n",
    "    \n",
    "    print(\"\\nTop 10 pivotal nodes (highest betweenness centrality):\")\n",
    "    for node_id, centrality in pivotal_nodes:\n",
    "        node_data = G.get_node_data(node_id)\n",
    "        print(f\"  {node_data[2]} ({node_data[0]}): {centrality:.4f}\")\n",
    "\n",
    "    return {\"G\": G, \"data_results\": {\"degrees\": degrees, \"communities\": communities, \"clustering_coeffs\": clustering_coeffs, \"betweenness\": betweenness}}\n",
    "    # Here you could add more analysis, such as:\n",
    "    # - Identify common attack patterns\n",
    "    # - Find relationships between different intrusion sets\n",
    "    # - Analyze the structure of specific sub-techniques\n",
    "\n",
    "G, node_map = create_knowledge_graph(file_path=file_path)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analyses\n",
    "    top_attack_patterns = analyze_top_attack_patterns(G, node_map)\n",
    "    hidden_relationships = analyze_hidden_relationships(G, node_map)\n",
    "    df_analysis = prepare_df_analysis(G, node_map)\n",
    "    \n",
    "    # Print additional analyses\n",
    "    print(\"\\nTop 5 Attack Patterns:\")\n",
    "    for pattern in top_attack_patterns:\n",
    "        print(f\"  - {pattern}\")\n",
    "\n",
    "    print(\"\\nHidden Relationships Between Intrusion Sets:\")\n",
    "    for relationship in hidden_relationships:\n",
    "        print(f\"  - {relationship}\")\n",
    "\n",
    "    print(\"\\nDataFrame Analysis for A Priori Knowledge for Sampling:\")\n",
    "    print(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Analysis\n",
    "# Here you could add more analysis, such as:\n",
    "    # - Identify common attack patterns\n",
    "    # - Find relationships between different intrusion sets\n",
    "    # - Analyze the structure of specific sub-techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate node degrees for all nodes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m degrees \u001b[38;5;241m=\u001b[39m [G\u001b[38;5;241m.\u001b[39mdegree(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[43mG\u001b[49m\u001b[38;5;241m.\u001b[39mnode_indices()]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Identify communities using connected components or any other community detection method\u001b[39;00m\n\u001b[1;32m      5\u001b[0m communities \u001b[38;5;241m=\u001b[39m {i: \u001b[38;5;28mlist\u001b[39m(comp) \u001b[38;5;28;01mfor\u001b[39;00m i, comp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rx\u001b[38;5;241m.\u001b[39mconnected_components(G))}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    " # Calculate node degrees for all nodes\n",
    "degrees = [G.degree(node) for node in G.node_indices()]\n",
    "\n",
    "# Identify communities using connected components or any other community detection method\n",
    "communities = {i: list(comp) for i, comp in enumerate(rx.connected_components(G))}\n",
    "\n",
    "# Calculate clustering coefficients for all nodes\n",
    "clustering_coeffs = list(rx.graph_all_clustering_coefficients(G).values())\n",
    "\n",
    "# Calculate betweenness centrality for all nodes\n",
    "betweenness = rx.graph_betweenness_centrality(G)\n",
    "\n",
    "return degrees, communities, clustering_coeffs, betweenness\n",
    "\n",
    "def identify_pivotal_nodes(G: rx.PyGraph, betweenness: Dict[int, float]) -> List[Tuple[int, float]]:\n",
    "    \"\"\"\n",
    "    Identifies pivotal nodes in the graph based on betweenness centrality.\n",
    "\n",
    "    Args:\n",
    "        G (rx.PyGraph): The knowledge graph.\n",
    "        betweenness (Dict[int, float]): Betweenness centrality values for nodes.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, float]]: List of top 10 nodes with highest betweenness centrality.\n",
    "    \"\"\"\n",
    "    # Sort nodes by their betweenness centrality to find pivotal nodes\n",
    "    sorted_nodes = sorted(betweenness.items(), key=lambda x: x[1], reverse=True)\n",
    "    # Return the top 10 pivotal nodes\n",
    "    return sorted_nodes[:10]\n",
    "\n",
    "def main(file_path: str):\n",
    "    # Load MITRE ATT&CK data\n",
    "    data = load_mitre_data(file_path=file_path)\n",
    "\n",
    "    # Create the knowledge graph\n",
    "    G, node_map = create_knowledge_graph(data)\n",
    "\n",
    "    # Analyze the graph\n",
    "    degrees, communities, clustering_coeffs, betweenness = analyze_graph(G)\n",
    "\n",
    "    # Identify pivotal nodes\n",
    "    pivotal_nodes = identify_pivotal_nodes(G, betweenness)\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"Number of nodes: {G.num_nodes()}\")\n",
    "    print(f\"Number of edges: {G.num_edges()}\")\n",
    "    print(f\"Average degree: {sum(degrees) / len(degrees):.2f}\")\n",
    "    print(f\"Number of communities: {len(communities)}\")\n",
    "    print(f\"Average clustering coefficient: {sum(clustering_coeffs) / len(clustering_coeffs):.4f}\")\n",
    "\n",
    "    print(\"\\nTop 10 pivotal nodes (highest betweenness centrality):\")\n",
    "    for node_id, centrality in pivotal_nodes:\n",
    "        node_data = G[node_id]\n",
    "        print(f\"  Node {node_id}: {node_data} with centrality {centrality:.4f}\")\n",
    "\n",
    "    # Additional analyses\n",
    "    top_attack_patterns = analyze_top_attack_patterns(G, node_map)\n",
    "    hidden_relationships = analyze_hidden_relationships(G, node_map)\n",
    "    df_analysis = prepare_df_analysis(G, node_map)\n",
    "\n",
    "    # Print additional analyses\n",
    "    print(\"\\nTop 5 Attack Patterns:\")\n",
    "    for pattern in top_attack_patterns:\n",
    "        print(f\"  - {pattern}\")\n",
    "\n",
    "    print(\"\\nHidden Relationships Between Intrusion Sets:\")\n",
    "    for relationship in hidden_relationships:\n",
    "        print(f\"  - {relationship}\")\n",
    "\n",
    "    print(\"\\nDataFrame Analysis for A Priori Knowledge for Sampling:\")\n",
    "    print(df_analysis)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(file_path=file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
